{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart to Langchain with OpenAI\n",
    "In this notebook, we'll look to get up & running with LangChain and OpenAI.\n",
    "\n",
    "To use OpenAI API, you'll need to sign up for the API access, which is a paid service and acquire an API key. For instructions [see this link](). Generate a new API token and save it in a local `.env` file as\n",
    "```\n",
    "OPENAI_API_KEY=sk-XXXXXX\n",
    "````\n",
    "Replace `sk-XXXXXX` with your API key. **Don't share this with ANYONE**. Add `.env` to the `.gitignore` file, so that this file is never uploaded to GitHub (assuming you are using Git for version control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# load API keys from local .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM instance\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepika Padukone's birthday is on January 5th. She was born in 1986. The cricket world cup in 1986 was won by India.\n"
     ]
    }
   ],
   "source": [
    "# modify the question below & try\n",
    "question = \"\"\"\n",
    "What is Deepika Padukone's birthday? Which country won the cricket world cup the year she was born?\n",
    "\"\"\"\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's us a prompt template instead\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Red, Amber, Green) status indicators can be a helpful tool for LLMs (Large Language Models) when dealing with Q&A (Question and Answer) tasks. Here's how RAG can assist LLMs with Q&A:\n",
      "\n",
      "1. **Prioritization**: By using RAG indicators, LLMs can prioritize their responses based on the urgency or importance of the questions. Questions marked as red (critical) can be addressed first, followed by amber (important) and green (less urgent) questions.\n",
      "\n",
      "2. **Quality Control**: LLMs can use RAG indicators to assess the quality of their responses. Red flags can indicate potential inaccuracies or gaps in information that need to be addressed before providing the final answer.\n",
      "\n",
      "3. **Efficiency**: RAG can help LLMs streamline their workflow by quickly identifying which questions require immediate attention and which ones can be addressed later. This can improve efficiency and ensure that time is spent on the most critical tasks.\n",
      "\n",
      "4. **Feedback Loop**: By using RAG indicators, LLMs can track their performance over time and identify patterns in the types of questions that are frequently marked as red or amber. This feedback loop can help LLMs improve their responses and enhance their overall performance in Q&A tasks.\n",
      "\n",
      "Overall, RAG status indicators can be a valuable tool for LLMs to manage and prioritize their Q&A tasks effectively, improve the quality of their responses, and optimize their workflow for maximum efficiency.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "response = chain.invoke({\"input\": \"How can RAG help LLMs with Q&A?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Red, Amber, Green) status indicators can be a helpful tool for LLMs (Large Language Models) when dealing with Q&A (Question and Answer) tasks. Here's how RAG can assist LLMs with Q&A:\n",
      "\n",
      "1. **Red**: When a question is marked as \"Red,\" it indicates that the LLM has low confidence in the answer provided. This can prompt the LLM to either seek more information or provide a disclaimer that the answer may not be accurate.\n",
      "\n",
      "2. **Amber**: An \"Amber\" status signifies moderate confidence in the answer. The LLM can use this as a signal to double-check the information before presenting it as a final response.\n",
      "\n",
      "3. **Green**: A \"Green\" status indicates high confidence in the answer provided by the LLM. This can give assurance to the user that the information is likely to be accurate.\n",
      "\n",
      "By using RAG status indicators, LLMs can better manage the quality of their responses during Q&A tasks, leading to more reliable and trustworthy information being provided to users.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"How can RAG help LLMs with Q&A?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us query a website for the ICC Cricket World Cup 1987, which was co-hosted by India & Pakistan, and ask some questions. By default, our LLM _may not_ have this knowledge. So we will help it by adding additional context to our query using RAG.\n",
    "In this process, we'll look up relevant documents/information using a _Retriever_ class, which is then passed to our LLM as additional context. We'll populate a vector store (local storage) and use that as a retriever.\n",
    "\n",
    "Please ensure that you have installed `beautifulsoup4` to help parse data from a webpage.\n",
    "```bash\n",
    "$> pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# load the documents - this code fragment just loads the documents. It\n",
    "# does not vectorize anything\n",
    "world_cup_url = \"https://en.wikipedia.org/wiki/1987_Cricket_World_Cup\"\n",
    "loader = WebBaseLoader(world_cup_url)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from local vector store\n"
     ]
    }
   ],
   "source": [
    "# here is the code to embed the web page into a local vector store\n",
    "import os, pathlib\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# create directory to save vector store\n",
    "save_path = pathlib.Path(os.getcwd()) / \"vecstore\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "vec_store_path = save_path / \"faiss_index\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "if vec_store_path.exists():\n",
    "    # if already created, just load it\n",
    "    print(\"Loading from local vector store\")\n",
    "    vector = FAISS.load_local(\n",
    "        str(vec_store_path), embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    # create & save it\n",
    "    print(\"Creating new vector store from URL...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    vector = FAISS.from_documents(documents, embeddings)\n",
    "    vector.save_local(str(vec_store_path))\n",
    "    print(f\"Vector store saved to {str(vec_store_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt_wc = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_wc)\n",
    "\n",
    "# create a retrieval chain to get the answer\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The participating countries were divided into two groups. India played in Group A along with Australia, New Zealand, and Zimbabwe.\n"
     ]
    }
   ],
   "source": [
    "# modify the question below & try\n",
    "wc_question = \"\"\"\n",
    "How many groups were the participating countries divided into. In which group did India play \n",
    "and who were the other countries in that group?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England played against Pakistan, West Indies, and Sri Lanka in the Group B matches. The results of those matches were as follows:\n",
      "- England vs Pakistan: Australia won by 18 runs\n",
      "- England vs West Indies: England won\n",
      "- England vs Sri Lanka: England won\n"
     ]
    }
   ],
   "source": [
    "wc_question2 = \"\"\"\n",
    "With which countries did England play in the Group B matches? What were the results of all \n",
    "those matches?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question2})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, the West Indies did not qualify for the semi-finals. The semi-finals and finals were played in Lahore, Calcutta, and Bombay, India. The teams that played in the semi-finals were Australia, Pakistan, England, and India. Australia won the finals against England by seven runs.\n"
     ]
    }
   ],
   "source": [
    "wc_question3 = \"\"\"\n",
    "Did the West Indies qualify for the semi finals? Where were the semi-finals and finals played. \n",
    "Who were the teams that played the semi-finals and finals? Who won the finals and by how many runs?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question3})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two leading run scorers were Graham Gooch from England with 471 runs and David Boon from Australia with 447 runs. The two leading wicket takers were Craig McDermott from Australia with 18 wickets and Imran Khan from Pakistan with 17 wickets.\n"
     ]
    }
   ],
   "source": [
    "wc_question4 = \"\"\"\n",
    "Who were the two leading run scorers and the two leading wicket takers?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question4})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
