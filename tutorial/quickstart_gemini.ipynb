{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart to Langchain with Google Gemini\n",
    "In this notebook, we'll look to get up & running with LangChain and Google Gemini.\n",
    "\n",
    "To use Gemini API you'll need to sign up for the API access, which is still free by the way (at least as of June '24) - Thanks Google!, and acquire an API key. For instructions [see this link](). Generate a new API token and save it in a local `.env` file as\n",
    "```\n",
    "OPENAI_API_KEY=sk-XXXXXX\n",
    "````\n",
    "Replace `sk-XXXXXX` with your API key. **Don't share this with ANYONE**. Add `.env` to the `.gitignore` file, so that this file is never uploaded to GitHub (assuming you are using Git for version control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# load API keys from local .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM instance\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # we will be using just the text model\n",
    "    model=\"gemini-1.5-flash\",  # \"gemini-pro\",\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    # No creativity whatsoever!\n",
    "    temperature=0.6,\n",
    "    # LangChain hack as Gemini does not support system prompts\n",
    "    # as a separate category, though we can set the context using\n",
    "    # a human prompt as a system prompt\n",
    "    # convert_system_message_to_human=True,   ##will be deprecated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepika Padukone's birthday is **January 5th**.\n",
      "\n",
      "She was born in 1986. In 1986, **Australia** won the Cricket World Cup. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modify the question below & try\n",
    "question = \"\"\"\n",
    "What is Deepika Padukone's birthday? Which country won the cricket world cup the year she was born?\n",
    "\"\"\"\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's us a prompt template instead\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # here is where the convert_system_message_to_human=True,\n",
    "        # parameter to the ChatGoogleGenerativeAI() constructor\n",
    "        # becomes relevant\n",
    "        (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Retrieval Augmented Generation (RAG) for Enhanced LLM Q&A: A Powerful Partnership\n",
      "\n",
      "Retrieval Augmented Generation (RAG) empowers Large Language Models (LLMs) to excel in Question Answering (Q&A) by bridging the gap between vast knowledge bases and their inherent limitations. Here's how RAG elevates LLM Q&A performance:\n",
      "\n",
      "**1. Accessing External Knowledge:**\n",
      "\n",
      "* **Beyond Pre-training:** LLMs are trained on massive datasets, but their knowledge is limited to that training data. RAG allows them to access external knowledge sources like databases, documents, or websites, expanding their information pool.\n",
      "* **Dynamic Knowledge:** RAG enables LLMs to answer questions based on the most up-to-date information. This is crucial for domains like news, finance, or scientific research where knowledge is constantly evolving.\n",
      "\n",
      "**2. Contextualized Responses:**\n",
      "\n",
      "* **Relevant Information:** RAG retrieves relevant information from external sources based on the question's context. This allows LLMs to provide more accurate and specific answers instead of relying on general knowledge.\n",
      "* **Evidence-based Answers:** RAG can provide evidence from the retrieved sources, supporting its answers and enhancing trust in the LLM's responses.\n",
      "\n",
      "**3. Improved Accuracy and Consistency:**\n",
      "\n",
      "* **Fact Verification:** RAG helps LLMs verify factual information by comparing their generated answers with retrieved sources. This reduces the risk of hallucinations and promotes more reliable responses.\n",
      "* **Consistency and Coherence:** By grounding answers in external knowledge, RAG ensures consistency and coherence in the LLM's responses, making them more trustworthy and reliable.\n",
      "\n",
      "**4. Specialized Knowledge Domains:**\n",
      "\n",
      "* **Domain-specific Expertise:** RAG allows LLMs to specialize in specific domains by accessing domain-specific knowledge sources like medical journals, legal databases, or financial reports.\n",
      "* **Personalized Responses:** RAG can tailor responses to individual users based on their specific needs and interests by accessing personalized knowledge sources.\n",
      "\n",
      "**5. Enhanced User Experience:**\n",
      "\n",
      "* **More Informative Responses:** RAG enables LLMs to provide more comprehensive and informative answers by incorporating relevant information from external sources.\n",
      "* **Improved User Engagement:** By providing evidence-based and contextually relevant responses, RAG increases user engagement and satisfaction with LLM-powered Q&A systems.\n",
      "\n",
      "**In conclusion, RAG empowers LLMs to become more powerful and versatile Q&A tools by bridging the gap between their inherent knowledge and the vast amount of information available in the real world. This partnership unlocks new possibilities for LLM applications across diverse domains, offering enhanced accuracy, relevance, and user experience.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "response = chain.invoke(\n",
    "    {\"input\": \"How can Retrieval Augmented Generation(RAG) help LLMs with Q&A?\"}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output above has lots of markdown. We can display the markdown in the output cell of a notebook using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> ## Retrieval Augmented Generation (RAG) for Enhanced LLM Q&A: A Powerful Partnership\n",
       "> \n",
       "> Retrieval Augmented Generation (RAG) empowers Large Language Models (LLMs) to excel in Question Answering (Q&A) by bridging the gap between vast knowledge bases and their inherent limitations. Here's how RAG elevates LLM Q&A performance:\n",
       "> \n",
       "> **1. Accessing External Knowledge:**\n",
       "> \n",
       "> * **Beyond Pre-training:** LLMs are trained on massive datasets, but their knowledge is limited to that training data. RAG allows them to access external knowledge sources like databases, documents, or websites, expanding their information pool.\n",
       "> * **Dynamic Knowledge:** RAG enables LLMs to answer questions based on the most up-to-date information. This is crucial for domains like news, finance, or scientific research where knowledge is constantly evolving.\n",
       "> \n",
       "> **2. Contextualized Responses:**\n",
       "> \n",
       "> * **Relevant Information:** RAG retrieves relevant information from external sources based on the question's context. This allows LLMs to provide more accurate and specific answers instead of relying on general knowledge.\n",
       "> * **Evidence-based Answers:** RAG can provide evidence from the retrieved sources, supporting its answers and enhancing trust in the LLM's responses.\n",
       "> \n",
       "> **3. Improved Accuracy and Consistency:**\n",
       "> \n",
       "> * **Fact Verification:** RAG helps LLMs verify factual information by comparing their generated answers with retrieved sources. This reduces the risk of hallucinations and promotes more reliable responses.\n",
       "> * **Consistency and Coherence:** By grounding answers in external knowledge, RAG ensures consistency and coherence in the LLM's responses, making them more trustworthy and reliable.\n",
       "> \n",
       "> **4. Specialized Knowledge Domains:**\n",
       "> \n",
       "> * **Domain-specific Expertise:** RAG allows LLMs to specialize in specific domains by accessing domain-specific knowledge sources like medical journals, legal databases, or financial reports.\n",
       "> * **Personalized Responses:** RAG can tailor responses to individual users based on their specific needs and interests by accessing personalized knowledge sources.\n",
       "> \n",
       "> **5. Enhanced User Experience:**\n",
       "> \n",
       "> * **More Informative Responses:** RAG enables LLMs to provide more comprehensive and informative answers by incorporating relevant information from external sources.\n",
       "> * **Improved User Engagement:** By providing evidence-based and contextually relevant responses, RAG increases user engagement and satisfaction with LLM-powered Q&A systems.\n",
       "> \n",
       "> **In conclusion, RAG empowers LLMs to become more powerful and versatile Q&A tools by bridging the gap between their inherent knowledge and the vast amount of information available in the real world. This partnership unlocks new possibilities for LLM applications across diverse domains, offering enhanced accuracy, relevance, and user experience.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace(\"•\", \"  *\")\n",
    "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))\n",
    "\n",
    "\n",
    "# instead of print(response.content), use\n",
    "to_markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! That's much better. Now let's see what effect adding an output parser, such as `StrOutputParser()` has on the output end of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we string them together - prompt -> llm -> output parser\n",
    "chain2 = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Retrieval Augmented Generation (RAG) for Enhanced LLM Q&A: Bridging the Gap\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a powerful technique that significantly enhances the capabilities of Large Language Models (LLMs) in Question Answering (Q&A) tasks. By combining the strengths of both retrieval and generation, RAG empowers LLMs to provide more accurate, relevant, and informative answers. \n",
      "\n",
      "Here's how RAG empowers LLMs in Q&A:\n",
      "\n",
      "**1. Accessing External Knowledge:**\n",
      "\n",
      "* **Beyond Pre-training:** LLMs are trained on massive datasets, but their knowledge is limited to the information they were exposed to during training. RAG allows LLMs to access external knowledge bases, like databases, documents, or websites, to retrieve relevant information in real-time.\n",
      "* **Dynamic Knowledge:** RAG enables LLMs to answer questions based on the latest information, even if it was not available during training. This is crucial for domains with rapidly evolving knowledge, such as finance or technology.\n",
      "* **Contextualized Answers:** By retrieving relevant information from external sources, RAG allows LLMs to provide answers that are grounded in specific contexts, making them more reliable and informative.\n",
      "\n",
      "**2. Improved Accuracy and Relevance:**\n",
      "\n",
      "* **Reduced Hallucinations:** LLMs sometimes generate plausible but incorrect answers, known as hallucinations. RAG mitigates this by grounding the LLM's responses in factual information retrieved from external sources.\n",
      "* **Enhanced Relevance:** RAG helps LLMs provide more relevant answers by retrieving information specifically related to the user's query, rather than relying solely on general knowledge.\n",
      "* **Factual Validation:** By providing evidence from external sources, RAG allows users to verify the accuracy of the LLM's responses, increasing trust and confidence.\n",
      "\n",
      "**3. Enhanced Explainability:**\n",
      "\n",
      "* **Traceability:** RAG provides a clear chain of reasoning, allowing users to understand how the LLM arrived at its answer. This transparency is crucial for building trust and understanding the limitations of the technology.\n",
      "* **Evidence-Based Responses:** RAG enables LLMs to provide evidence from external sources to support their answers, making them more convincing and persuasive.\n",
      "\n",
      "**4. Expanding Capabilities:**\n",
      "\n",
      "* **Multi-Modal Q&A:** RAG can be extended to handle multi-modal queries, such as questions involving images, videos, or audio. This opens up new possibilities for interactive and engaging Q&A systems.\n",
      "* **Personalized Responses:** RAG can be used to tailor responses to individual users based on their preferences and past interactions. This can lead to a more personalized and engaging Q&A experience.\n",
      "\n",
      "**Examples of RAG in Action:**\n",
      "\n",
      "* **Customer Support:** RAG can be used to provide accurate and efficient customer support, by retrieving information from knowledge bases or FAQs to answer customer questions.\n",
      "* **Research:** RAG can assist researchers by retrieving relevant scientific articles or data to support their research.\n",
      "* **Education:** RAG can be used to create personalized learning experiences, providing students with tailored answers and explanations based on their specific needs.\n",
      "\n",
      "**In conclusion, RAG is a transformative technology that significantly enhances the capabilities of LLMs in Q&A. By bridging the gap between internal knowledge and external information, RAG empowers LLMs to provide more accurate, relevant, and informative answers, leading to a more reliable and engaging Q&A experience.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and voila\n",
    "response = chain.invoke(\n",
    "    {\"input\": \"How can Retrieval Augmented Generation(RAG) help LLMs with Q&A?\"}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us query a website for the ICC Cricket World Cup 1987, which was co-hosted by India & Pakistan, and ask some questions. By default, our LLM _may not_ have this knowledge. So we will help it by adding additional context to our query using RAG.\n",
    "In this process, we'll look up relevant documents/information using a _Retriever_ class, which is then passed to our LLM as additional context. We'll populate a vector store (local storage) and use that as a retriever.\n",
    "\n",
    "Please ensure that you have installed `beautifulsoup4` to help parse data from a webpage.\n",
    "```bash\n",
    "$> pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# load the documents - this code fragment just loads the documents. It\n",
    "# does not vectorize anything\n",
    "world_cup_url = \"https://en.wikipedia.org/wiki/1987_Cricket_World_Cup\"\n",
    "loader = WebBaseLoader(world_cup_url)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store from URL...\n",
      "Vector store saved to c:\\Dev\\Code\\git-projects\\LangChain\\tutorial\\vecstore\\faiss_index_gemini\n"
     ]
    }
   ],
   "source": [
    "# here is the code to embed the web page into a local vector store\n",
    "import os, pathlib\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# create directory to save vector store\n",
    "save_path = pathlib.Path(os.getcwd()) / \"vecstore\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "vec_store_path = save_path / \"faiss_index_gemini\"\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "if vec_store_path.exists():\n",
    "    # if already created, just load it\n",
    "    print(\"Loading from local vector store\")\n",
    "    vector = FAISS.load_local(\n",
    "        str(vec_store_path), embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    # create & save it\n",
    "    print(\"Creating new vector store from URL...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    vector = FAISS.from_documents(documents, embeddings)\n",
    "    vector.save_local(str(vec_store_path))\n",
    "    print(f\"Vector store saved to {str(vec_store_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt_wc = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_wc)\n",
    "\n",
    "# create a retrieval chain to get the answer\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The participating countries were divided into **two** groups. India played in **Group A** along with **Australia, New Zealand, and Zimbabwe**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modify the question below & try\n",
    "wc_question = \"\"\"\n",
    "How many groups were the participating countries divided into. In which group did India play \n",
    "and who were the other countries in that group?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> England played against Pakistan, West Indies and Sri Lanka in the Group B matches. Here are the results:\n",
       "> \n",
       "> * **England vs West Indies:** England won by 2 wickets.\n",
       "> * **England vs Pakistan:** Pakistan won by 18 runs.\n",
       "> * **England vs Sri Lanka:** England won by 108 runs.\n",
       "> * **England vs Pakistan:** Pakistan won by 7 wickets. \n",
       "> * **England vs West Indies:** England won by 34 runs.\n",
       "> * **England vs Sri Lanka:** England won by 8 wickets. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_question2 = \"\"\"\n",
    "With which countries did England play in the Group B matches? What were the results of all \n",
    "those matches?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question2})\n",
    "to_markdown(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The West Indies did not qualify for the semi-finals. \n",
      "\n",
      "The semi-finals were played in Lahore, Pakistan, and Bombay, India. The finals were played in Calcutta, India.\n",
      "\n",
      "The semi-finals were played between:\n",
      "* Australia vs Pakistan\n",
      "* England vs India\n",
      "\n",
      "The finals were played between Australia and England. Australia won the finals by 7 runs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wc_question3 = \"\"\"\n",
    "Did the West Indies qualify for the semi finals? Where were the semi-finals and finals played. \n",
    "Who were the teams that played the semi-finals and finals? Who won the finals and by how many runs?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question3})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two leading run scorers were Graham Gooch (England) and David Boon (Australia). The two leading wicket takers were Craig McDermott (Australia) and Imran Khan (Pakistan). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wc_question4 = \"\"\"\n",
    "Who were the two leading run scorers and the two leading wicket takers?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question4})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
