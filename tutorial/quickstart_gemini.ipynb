{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart to Langchain with Google Gemini\n",
    "In this notebook, we'll look to get up & running with LangChain and Google Gemini.\n",
    "\n",
    "To use Gemini API you'll need to sign up for the API access, which is still free by the way (at least as of June '24) - Thanks Google!, and acquire an API key. For instructions [see this link](). Generate a new API token and save it in a local `.env` file as\n",
    "```\n",
    "OPENAI_API_KEY=sk-XXXXXX\n",
    "````\n",
    "Replace `sk-XXXXXX` with your API key. **Don't share this with ANYONE**. Add `.env` to the `.gitignore` file, so that this file is never uploaded to GitHub (assuming you are using Git for version control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# load API keys from local .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM instance\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # we will be using just the text model\n",
    "    model=\"gemini-pro\",\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    # No creativity whatsoever!\n",
    "    temperature=0.0,\n",
    "    # LangChain hack as Gemini does not support system prompts\n",
    "    # as a separate category, though we can set the context using\n",
    "    # a human prompt as a system prompt\n",
    "    convert_system_message_to_human=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Deepika Padukone's birthday: **January 5, 1986**\n",
      "* Country that won the cricket world cup in 1986: **Australia**\n"
     ]
    }
   ],
   "source": [
    "# modify the question below & try\n",
    "question = \"\"\"\n",
    "What is Deepika Padukone's birthday? Which country won the cricket world cup the year she was born?\n",
    "\"\"\"\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's us a prompt template instead\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # here is where the convert_system_message_to_human=True,\n",
    "        # parameter to the ChatGoogleGenerativeAI() constructor\n",
    "        # becomes relevant\n",
    "        (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**RAG (Retrieval-Augmented Generation)** can significantly enhance the Q&A capabilities of Large Language Models (LLMs) by leveraging external knowledge sources. Here's how RAG can assist LLMs with Q&A:\n",
      "\n",
      "**1. Knowledge Retrieval:**\n",
      "RAG retrieves relevant information from external knowledge bases, such as Wikipedia, news articles, or domain-specific databases. This allows LLMs to access a broader range of information beyond their own training data.\n",
      "\n",
      "**2. Contextualization:**\n",
      "RAG provides LLMs with the context of the user's question by retrieving related documents. This helps LLMs understand the intent and specific requirements of the query.\n",
      "\n",
      "**3. Answer Generation:**\n",
      "LLMs use the retrieved knowledge to generate comprehensive and accurate answers. RAG ensures that the answers are grounded in factual information and relevant to the user's question.\n",
      "\n",
      "**4. Answer Refinement:**\n",
      "RAG can further refine the answers generated by LLMs by identifying and correcting potential errors or inconsistencies. This improves the quality and reliability of the Q&A output.\n",
      "\n",
      "**5. Knowledge Update:**\n",
      "RAG enables LLMs to continuously update their knowledge base by incorporating new information from external sources. This ensures that the Q&A system remains up-to-date and provides the most relevant answers.\n",
      "\n",
      "**Benefits of RAG for LLM Q&A:**\n",
      "\n",
      "* **Improved Accuracy:** RAG provides LLMs with access to factual information, reducing the risk of incorrect or biased answers.\n",
      "* **Enhanced Contextualization:** RAG helps LLMs understand the context of the question, leading to more relevant and specific answers.\n",
      "* **Broader Knowledge Base:** RAG expands the knowledge base of LLMs, allowing them to answer a wider range of questions.\n",
      "* **Continuous Learning:** RAG enables LLMs to continuously update their knowledge, ensuring the Q&A system remains current.\n",
      "* **Reduced Bias:** By leveraging external knowledge sources, RAG helps mitigate potential biases in LLM training data.\n",
      "\n",
      "In summary, RAG plays a crucial role in enhancing the Q&A capabilities of LLMs by providing them with relevant knowledge, contextualization, and answer refinement. This results in more accurate, comprehensive, and up-to-date answers, making LLMs more effective for Q&A tasks.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "response = chain.invoke({\"input\": \"How can RAG help LLMs with Q&A?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output above has lots of markdown. We can display the markdown in the output cell of a notebook using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **RAG (Retrieval-Augmented Generation)** can significantly enhance the Q&A capabilities of Large Language Models (LLMs) by leveraging external knowledge sources. Here's how RAG can assist LLMs with Q&A:\n",
       "> \n",
       "> **1. Knowledge Retrieval:**\n",
       "> RAG retrieves relevant information from external knowledge bases, such as Wikipedia, news articles, or domain-specific databases. This allows LLMs to access a broader range of information beyond their own training data.\n",
       "> \n",
       "> **2. Contextualization:**\n",
       "> RAG provides LLMs with the context of the user's question by retrieving related documents. This helps LLMs understand the intent and specific requirements of the query.\n",
       "> \n",
       "> **3. Answer Generation:**\n",
       "> LLMs use the retrieved knowledge to generate comprehensive and accurate answers. RAG ensures that the answers are grounded in factual information and relevant to the user's question.\n",
       "> \n",
       "> **4. Answer Refinement:**\n",
       "> RAG can further refine the answers generated by LLMs by identifying and correcting potential errors or inconsistencies. This improves the quality and reliability of the Q&A output.\n",
       "> \n",
       "> **5. Knowledge Update:**\n",
       "> RAG enables LLMs to continuously update their knowledge base by incorporating new information from external sources. This ensures that the Q&A system remains up-to-date and provides the most relevant answers.\n",
       "> \n",
       "> **Benefits of RAG for LLM Q&A:**\n",
       "> \n",
       "> * **Improved Accuracy:** RAG provides LLMs with access to factual information, reducing the risk of incorrect or biased answers.\n",
       "> * **Enhanced Contextualization:** RAG helps LLMs understand the context of the question, leading to more relevant and specific answers.\n",
       "> * **Broader Knowledge Base:** RAG expands the knowledge base of LLMs, allowing them to answer a wider range of questions.\n",
       "> * **Continuous Learning:** RAG enables LLMs to continuously update their knowledge, ensuring the Q&A system remains current.\n",
       "> * **Reduced Bias:** By leveraging external knowledge sources, RAG helps mitigate potential biases in LLM training data.\n",
       "> \n",
       "> In summary, RAG plays a crucial role in enhancing the Q&A capabilities of LLMs by providing them with relevant knowledge, contextualization, and answer refinement. This results in more accurate, comprehensive, and up-to-date answers, making LLMs more effective for Q&A tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace(\"â€¢\", \"  *\")\n",
    "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))\n",
    "\n",
    "\n",
    "# instead of print(response.content), use\n",
    "to_markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! That's much better. Now let's see what effect adding an output parser, such as `StrOutputParser()` has on the output end of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we string them together - prompt -> llm -> output parser\n",
    "chain2 = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**RAG (Retrieval-Augmented Generation)** can significantly enhance the Q&A capabilities of Large Language Models (LLMs) by leveraging external knowledge sources. Here's how RAG can assist LLMs with Q&A:\n",
      "\n",
      "**1. Knowledge Retrieval:**\n",
      "RAG retrieves relevant information from a large knowledge base, such as Wikipedia or a domain-specific corpus. This knowledge is used to augment the LLM's internal knowledge, providing it with a broader context and deeper understanding.\n",
      "\n",
      "**2. Contextualization:**\n",
      "RAG helps LLMs understand the context of the user's question by providing relevant passages or documents. This contextualization enables the LLM to generate more accurate and comprehensive answers.\n",
      "\n",
      "**3. Fact Verification:**\n",
      "RAG can verify the accuracy of the LLM's answers by comparing them to the retrieved knowledge. This ensures that the answers are factually correct and reliable.\n",
      "\n",
      "**4. Answer Generation:**\n",
      "RAG can generate answers by combining the retrieved knowledge with the LLM's own language generation capabilities. This hybrid approach produces answers that are both informative and well-written.\n",
      "\n",
      "**5. Answer Refinement:**\n",
      "RAG can refine the LLM's answers by identifying and correcting errors, improving clarity, and ensuring that the answers are concise and relevant.\n",
      "\n",
      "**Benefits of RAG for LLM Q&A:**\n",
      "\n",
      "* **Improved Accuracy:** RAG provides LLMs with access to external knowledge, reducing the risk of incorrect or biased answers.\n",
      "* **Enhanced Contextualization:** RAG helps LLMs understand the context of questions, leading to more relevant and comprehensive answers.\n",
      "* **Fact Verification:** RAG ensures the accuracy of answers by verifying them against external sources.\n",
      "* **Answer Refinement:** RAG helps LLMs refine their answers, making them more polished and informative.\n",
      "* **Scalability:** RAG can be integrated with any LLM, making it a versatile solution for Q&A applications.\n",
      "\n",
      "By leveraging RAG, LLMs can become more effective and reliable Q&A systems, providing users with accurate, contextualized, and well-written answers.\n"
     ]
    }
   ],
   "source": [
    "# and voila\n",
    "response = chain.invoke({\"input\": \"How can RAG help LLMs with Q&A?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us query a website for the ICC Cricket World Cup 1987, which was co-hosted by India & Pakistan, and ask some questions. By default, our LLM _may not_ have this knowledge. So we will help it by adding additional context to our query using RAG.\n",
    "In this process, we'll look up relevant documents/information using a _Retriever_ class, which is then passed to our LLM as additional context. We'll populate a vector store (local storage) and use that as a retriever.\n",
    "\n",
    "Please ensure that you have installed `beautifulsoup4` to help parse data from a webpage.\n",
    "```bash\n",
    "$> pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# load the documents - this code fragment just loads the documents. It\n",
    "# does not vectorize anything\n",
    "world_cup_url = \"https://en.wikipedia.org/wiki/1987_Cricket_World_Cup\"\n",
    "loader = WebBaseLoader(world_cup_url)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store from URL...\n",
      "Vector store saved to /home/mjbhobe/code/git-projects/LangChain/tutorial/vecstore/faiss_index_gemini\n"
     ]
    }
   ],
   "source": [
    "# here is the code to embed the web page into a local vector store\n",
    "import os, pathlib\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# create directory to save vector store\n",
    "save_path = pathlib.Path(os.getcwd()) / \"vecstore\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "vec_store_path = save_path / \"faiss_index_gemini\"\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "if vec_store_path.exists():\n",
    "    # if already created, just load it\n",
    "    print(\"Loading from local vector store\")\n",
    "    vector = FAISS.load_local(\n",
    "        str(vec_store_path), embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    # create & save it\n",
    "    print(\"Creating new vector store from URL...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    vector = FAISS.from_documents(documents, embeddings)\n",
    "    vector.save_local(str(vec_store_path))\n",
    "    print(f\"Vector store saved to {str(vec_store_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt_wc = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_wc)\n",
    "\n",
    "# create a retrieval chain to get the answer\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The participating countries were divided into two groups. India played in Group A, along with Australia, New Zealand, and Zimbabwe.\n"
     ]
    }
   ],
   "source": [
    "# modify the question below & try\n",
    "wc_question = \"\"\"\n",
    "How many groups were the participating countries divided into. In which group did India play \n",
    "and who were the other countries in that group?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not specify which countries England played in the Group B matches or the results of those matches.\n"
     ]
    }
   ],
   "source": [
    "wc_question2 = \"\"\"\n",
    "With which countries did England play in the Group B matches? What were the results of all \n",
    "those matches?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question2})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The West Indies did not qualify for the semi finals. \n",
      "\n",
      "The semi-finals were played in Lahore, Pakistan and Bombay, India. The finals were played in Calcutta, India. \n",
      "\n",
      "The semi-finals were played between Australia and Pakistan in Lahore, Pakistan and between England and India in Bombay, India. \n",
      "\n",
      "The finals were played between Australia and England in Calcutta, India. Australia won the finals by 7 runs.\n"
     ]
    }
   ],
   "source": [
    "wc_question3 = \"\"\"\n",
    "Did the West Indies qualify for the semi finals? Where were the semi-finals and finals played. \n",
    "Who were the teams that played the semi-finals and finals? Who won the finals and by how many runs?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question3})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbhobe/code/git-projects/LangChain/myenv/lib/python3.9/site-packages/langchain_google_genai/chat_models.py:352: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two leading run scorers were Graham Gooch (471 runs) and David Boon (447 runs). The two leading wicket takers were Craig McDermott (18 wickets) and Imran Khan (17 wickets).\n"
     ]
    }
   ],
   "source": [
    "wc_question4 = \"\"\"\n",
    "Who were the two leading run scorers and the two leading wicket takers?\n",
    "\"\"\"\n",
    "response = retrieval_chain.invoke({\"input\": wc_question4})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
