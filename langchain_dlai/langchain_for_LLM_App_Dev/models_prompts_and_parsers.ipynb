{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Models, Prompts and Output Parsers\n",
    "### Outline\n",
    "* Direct API calls to OpenAI\n",
    "* API calls through LangChain\n",
    "    * Prompts\n",
    "    * Models\n",
    "    * Output Parsers\n",
    "\n",
    "### Setup\n",
    "1. Install the OpenAI & dotenv Python package as follows\n",
    "```shell\n",
    "$> pip install openai dotenv\n",
    "```\n",
    "2. Create an OpenAI API key from the [OpenAI API Website](https://openai.com/blog/openai-api)\n",
    "3. Copy a new key and save it to a `.env` file as `OPENAI_API_KEY=sk-....` (replace `sk-....` with your key)\n",
    "\n",
    "Load the API key and relevant Python libaries as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use LLM Model gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "print(f\"Will use LLM Model {llm_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my chat client\n",
    "from openai import OpenAI\n",
    "\n",
    "CHAT_CLIENT = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "MODEL = \"gpt-3.5-turbo\"  # enough for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat API: OpenAI\n",
    "Let's start with direct calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_completion(prompt, model=llm_model):\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#         temperature=0,\n",
    "#     )\n",
    "#     return response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    user_prompt: str, client=CHAT_CLIENT, model=MODEL, get_tokens=False\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    if get_tokens:\n",
    "        # get tokens also\n",
    "        tokens = {\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "        }\n",
    "        return response.choices[0].message, tokens\n",
    "    else:\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The future of Generative AI is promising, as this technology continues to advance and evolve. Some potential developments that could shape the future of Generative AI include:\\n\\n1. Better understanding and control over the output: As Generative AI models become more sophisticated, researchers are working on ways to better understand and control the outputs generated by these algorithms. This includes developing techniques to improve the quality, diversity, and coherence of the generated content.\\n\\n2. Human-AI collaboration: In the future, we may see increased collaboration between humans and Generative AI systems. This could involve using AI to assist with creative tasks, such as generating ideas or providing inspiration, while humans provide feedback and direction to help refine the final output.\\n\\n3. Personalized content generation: Generative AI could be used to create personalized content tailored to individual preferences and needs. This could include generating customized product designs, personalized recommendations, or tailored marketing materials.\\n\\n4. Ethical considerations: As Generative AI becomes more prevalent, issues related to ethics and bias will need to be addressed. Researchers are already working on developing techniques to ensure that AI-generated content is fair, unbiased, and safe for use.\\n\\nOverall, the future of Generative AI holds great potential for revolutionizing various industries and opening up new possibilities for creativity and innovation. As researchers continue to push the boundaries of what is possible with this technology, we can expect to see even more exciting developments in the years to come.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"What is the future of Generative AI?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I am frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I need your help immediately, my friend.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose you are a customer support rep, who receives emails from customers in different languages, like French, Spanish etc. You'd like to first translate it to \"American English in a calm and respectful tone\" so you can process it further.\n",
    "\n",
    "While we will not cover this here, you may also want to type in a response to the email and have the LLM translate it back to the original language when responding to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email_french = \"\"\"\n",
    "Qu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro \n",
    "avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go \n",
    "de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque.  \n",
    "J'attends un échange immédiat pour le produit que j'ai commandé!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text \n",
      "that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Qu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro \n",
      "avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go \n",
      "de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque.  \n",
      "J'attends un échange immédiat pour le produit que j'ai commandé!!!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = f\"\"\"Translate the text \n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email_french}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='What is Colorado Retail? I ordered a MacBook M3 Pro with 16GB of RAM and 512GB of SSD but I was delivered a MacBook Air M2 with 8GB of RAM and 256GB of SSD!! And on top of that, I was charged for the M3 Pro and even Microsoft Office is missing. I expect an immediate exchange for the product I ordered!!!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat API: LangChain\n",
    "Let's see how we can do the same in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model_name=MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email_french = \"\"\"\n",
    "Qu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro \n",
    "avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go \n",
    "de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque.  \n",
    "J'attends un échange immédiat pour le produit que j'ai commandé!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style,\n",
    "    text=customer_email_french,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nQu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro \\navec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go \\nde RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque.  \\nJ'attends un échange immédiat pour le produit que j'ai commandé!!!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Colorado Retail? I ordered a MacBook M3 Pro with 16GB of RAM and 512GB of SSD, but I was delivered a MacBook Air M2 with 8GB of RAM and 256GB SSD!! And on top of that, I was charged for the M3 Pro and even Microsoft Office is missing. I expect an immediate exchange for the product I ordered!!!\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)  # this should return the same response as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "We are really sorry to hear that we delivered \\\n",
    "the wrong device to you. We are looking into \\\n",
    "the matter and please rest assured that we will \\\n",
    "rectify the mistake at the earliest. \\\n",
    "Thank you for your loyal patronage, and we \\\n",
    "hope you will continue to shop with Colorado Retail \\\n",
    "for all your needs. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_french = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in French\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in French. text: ```Hey there customer, We are really sorry to hear that we delivered the wrong device to you. We are looking into the matter and please rest assured that we will rectify the mistake at the earliest. Thank you for your loyal patronage, and we hope you will continue to shop with Colorado Retail for all your needs. ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We just call the LangChain API\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_french, text=service_reply\n",
    ")\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour cher client,\n",
      "\n",
      "Nous sommes vraiment désolés d'apprendre que nous vous avons livré le mauvais appareil. Nous examinons la situation et vous pouvez être assuré que nous rectifierons l'erreur dès que possible. Merci pour votre fidélité, et nous espérons que vous continuerez à faire vos achats chez Colorado Retail pour tous vos besoins.\n",
      "\n",
      "Cordialement,\n"
     ]
    }
   ],
   "source": [
    "service_response = chat.invoke(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Output Parsers\n",
    "\n",
    "This is an example of using a JSON output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: response.content is a STRING, not a dict, though it looks like one\n",
    "# Now let us use output parsers to get true JSON\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now tell the parser what you want parsed out from output\n",
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased\\\n",
    "        as a gift for someone else? \\\n",
    "        Answer True if yes,\\\n",
    "        False if not or unknown.\",\n",
    ")\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days\\\n",
    "        did it take for the product\\\n",
    "        to arrive? If this \\\n",
    "        information is not found,\\\n",
    "        output -1.\",\n",
    ")\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "        sentences about the value or \\\n",
    "        price, and output them as a \\\n",
    "        comma separated Python list.\",\n",
    ")\n",
    "\n",
    "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased        as a gift for someone else?         Answer True if yes,        False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days        did it take for the product        to arrive? If this         information is not found,        output -1.\n",
      "\t\"price_value\": string  // Extract any        sentences about the value or         price, and output them as a         comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# show me what instructions LangChain generated\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try it out\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    text=customer_review, format_instructions=format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\ntext: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife\\'s anniversary present. I think my wife liked it so much she was speechless. So far I\\'ve been the only one using it, and I\\'ve been using it every other morning to clear the leaves on our lawn. It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\\n\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased        as a gift for someone else?         Answer True if yes,        False if not or unknown.\\n\\t\"delivery_days\": string  // How many days        did it take for the product        to arrive? If this         information is not found,        output -1.\\n\\t\"price_value\": string  // Extract any        sentences about the value or         price, and output them as a         comma separated Python list.\\n}\\n```\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> \n",
      " ```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(messages)\n",
    "print(type(response.content), \"\\n\", response.content)  # still a string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> \n",
      " {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n"
     ]
    }
   ],
   "source": [
    "# now it converts to a JSON\n",
    "output_dict = output_parser.parse(response.content)\n",
    "print(type(output_dict), \"\\n\", output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict[\"gift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
