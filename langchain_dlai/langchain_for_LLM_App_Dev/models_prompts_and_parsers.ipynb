{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Models, Prompts and Output Parsers\n",
    "### Outline\n",
    "* Direct API calls to OpenAI\n",
    "* API calls through LangChain\n",
    "    * Prompts\n",
    "    * Models\n",
    "    * Output Parsers\n",
    "\n",
    "### Setup\n",
    "1. Install the OpenAI & dotenv Python package as follows\n",
    "```shell\n",
    "$> pip install openai dotenv\n",
    "```\n",
    "2. Create an OpenAI API key from the [OpenAI API Website](https://openai.com/blog/openai-api)\n",
    "3. Copy a new key and save it to a `.env` file as `OPENAI_API_KEY=sk-....` (replace `sk-....` with your key)\n",
    "\n",
    "Load the API key and relevant Python libaries as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use LLM Model gpt-3.5-turbo-0301\n"
     ]
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "print(f\"Will use LLM Model {llm_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat API: OpenAI\n",
    "Let's start with direct calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot predict the future with certainty, but based on current trends and advancements in the field, the future of Generative AI looks promising. \n",
      "\n",
      "Generative AI has already shown its potential in various applications such as natural language processing, image and video generation, and music composition. With the development of more advanced algorithms and models, Generative AI is expected to become even more sophisticated and capable of producing more realistic and complex outputs.\n",
      "\n",
      "One of the most exciting areas of development in Generative AI is the use of Generative Adversarial Networks (GANs), which involve two neural networks working together to generate realistic outputs. GANs have already been used to create realistic images, videos, and even entire virtual worlds.\n",
      "\n",
      "Another area of development is the use of Generative AI in personalized content creation, such as personalized news articles, advertisements, and product recommendations. This could revolutionize the way we consume content and interact with brands.\n",
      "\n",
      "Overall, the future of Generative AI looks bright, and we can expect to see more innovative applications and advancements in the coming years.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"What is the future of Generative AI?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite upset that my blender lid came off and caused my smoothie to splatter all over my kitchen walls. Additionally, the warranty does not cover the cost of cleaning up the mess. Would you be able to assist me at this time, my friend?\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose you are a customer support rep, who receives emails from customers in different languages, like French, Spanish etc. You'd like to first translate it to \"American English in a calm and respectful tone\" so you can process it further.\n",
    "\n",
    "While we will not cover this here, you may also want to type in a response to the email and have the LLM translate it back to the original language when responding to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email_french = \"\"\"\n",
    "Qu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro \\\n",
    "avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go \\\n",
    "de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque. \\ \n",
    "J'attends un échange immédiat pour le produit que j'ai commandé!!!\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Qu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque. \\ \n",
      "J'attends un échange immédiat pour le produit que j'ai commandé!!!```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email_french}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Colorado Retail? I ordered a MacBook M3 Pro with 16GB of RAM and 512GB of SSD, but I received a MacBook Air M2 with 8GB of RAM and 256GB of SSD! Additionally, I was charged for the M3 Pro and Microsoft Office is missing. I am requesting an immediate exchange for the product I ordered. Thank you for your attention to this matter.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat API: LangChain\n",
    "Let's see how we can do the same in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the variables used in the message\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email_french = \"\"\"\n",
    "Qu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro \\\n",
    "avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go \\\n",
    "de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque. \\ \n",
    "J'attends un échange immédiat pour le produit que j'ai commandé!!!\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style,\n",
    "    text=customer_email_french,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nQu'est-ce que c'est que Colorado Retail ? J'avais commandé un MacBook M3 pro avec 16 Go de RAM et 512 Go de SSD mais on m'a livré un MacBook Air M2 avec 8 Go de RAM et 256 SSD !! Et en plus vous m'avez facturé le M3 Pro et même Microsoft Office manque. \\\\ \\nJ'attends un échange immédiat pour le produit que j'ai commandé!!!```\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What is Colorado Retail? I ordered a MacBook M3 Pro with 16GB of RAM and 512GB of SSD, but instead, I received a MacBook Air M2 with only 8GB of RAM and 256GB of SSD! To make matters worse, I was charged for the M3 Pro and Microsoft Office is missing. I am requesting an immediate exchange for the product I ordered.\"\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)  # this should return the same response as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "We are really sorry to hear that we delivered \\\n",
    "the wrong device to you. We are looking into \\\n",
    "the matter and please rest assured that we will \\\n",
    "rectify the mistake at the earliest. \\\n",
    "Thank you for your loyal patronage, and we \\\n",
    "hope you will continue to shop with Colorado Retail \\\n",
    "for all your needs. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_french = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in French\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in French. text: ```Hey there customer, We are really sorry to hear that we delivered the wrong device to you. We are looking into the matter and please rest assured that we will rectify the mistake at the earliest. Thank you for your loyal patronage, and we hope you will continue to shop with Colorado Retail for all your needs. ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We just call the LangChain API\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_french, text=service_reply\n",
    ")\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour cher client, nous sommes vraiment désolés d'apprendre que nous vous avons livré le mauvais appareil. Nous examinons la question et veuillez être assuré que nous rectifierons l'erreur dès que possible. Merci pour votre fidèle patronage, et nous espérons que vous continuerez à faire vos achats chez Colorado Retail pour tous vos besoins.\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
