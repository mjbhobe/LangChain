{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Recommendation wth OpenAI &amp; Langchain\n",
    "In ths notebook we will progressively develop code that leverages OpenAI to recommend a fancy name for a restaurant, then chain that recommended name to get a menu of items from OpenAI.\n",
    "\n",
    "## Step1: Setting up OpenAI\n",
    "As first step, you need to login to OpenAI API website and get an API key. I have stored mine in a file `c:\\dev\\OpenAIKey.txt`, which is **outside** any Github managed folders. You **don't want to accidentally upload your API key to GitHub**. OpenAI API requires that you set an environment variable `OPENAI_API_KEY` before calling any OpenAI APIs. This is what we'll do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the OpenAI API Key (read from file)\n",
    "OPEN_API_KEY_FILE = r\"c:\\dev\\OpenAIKey.txt\"\n",
    "if not os.path.exists(OPEN_API_KEY_FILE):\n",
    "    raise FileNotFoundError(f\"OpenAI API key file not found at {OPEN_API_KEY_FILE}\")\n",
    "\n",
    "OPEN_API_KEY = None\n",
    "with open(OPEN_API_KEY_FILE, \"r\") as f:\n",
    "    OPEN_API_KEY = f.read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - calling OpenAI API\n",
    "There are several Large Language Models (LLMs) that you can use - both commercial (such as OpenAI) and OpenSource (such as Bloom from Hugging Face). In this example we'll use LangChain to orchestrate the calls to OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - create our LLM\n",
    "\n",
    "llm = OpenAI(temperature=0.6)\n",
    "# the tempature parameter determines how 'creative' the LLM output is - it goes from 0 to 1, with 1 = most\n",
    "# creative and 0 = least creative. I usually use a value = 0.6 or 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".\\n\\nGolden Dragon Palace, Jade Phoenix Restaurant, Imperial Palace Cuisine, Fortune's Feast, Dynasty Delight.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 - ask the llm for 5 fancy names for an Chinse restaurant for example\n",
    "response = llm.predict(\n",
    "    \"Please suggest a fancy name for a Chinese restaurant. Return 5 comma separated names please\"\n",
    ")\n",
    "response\n",
    "# response could be soething like\n",
    "# \"\\n\\n\"Golden Dragon Palace, Jade Phoenix Restaurant, Imperial Palace Cuisine, Fortune's Feast, Dynasty Delight.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That was easy ðŸ˜ƒ. If you want a different cuisine, say Indian, American, Arabic etc., just replace the word Chinese with your prefered cuisine and ask again.\n",
    "\n",
    "## Step 3 - templatizing your prompt\n",
    "\n",
    "Do you see a problem here? The cuisine is _hard coded_ into the request, so each time you want to ask for a different type of cuisine, you have to change the prompt. What if we could _parameterize_ the request? That's where a framework such as `LangChain` comes in. We will use the `langchain.prompts.PromptTemplate` class to parameterize our request. A prompt has to be sent to an LLM, and the result fetched - all this can be chained together with an LLM chain, specifically `langchain.chains.LLMChain` class. Here is the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please suggest a fancy name for a Mexican restaurant. Return 5 comma separated names without leading numbers.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    # you can have multiple input variables, hence the list\n",
    "    input_variables=[\"cuisine\"],\n",
    "    # this is your prompt - same as above except the cuisine is parameterised with {}\n",
    "    # similar to a Python f-string\n",
    "    template=\"Please suggest a fancy name for a {cuisine} restaurant. Return 5 comma separated names without leading numbers.\",\n",
    "    verbose=True,\n",
    ")\n",
    "# here is how you format the template\n",
    "prompt_template_name.format(cuisine=\"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nGusto's Trattoria, La Dolce Vita Ristorante, La Tavola Cucina, Bella Cucina, Il Buon Cibo.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a chain to send the prompt to your LLM\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response = chain.run(cuisine=\"Italian\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gusto's Trattoria\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's grab just the first name\n",
    "rest_name = response.strip().split(\",\")[0]\n",
    "rest_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - chaining your requests together\n",
    "Now that we have a recommendation for our restaurant name, let's ask OpenAI to suggest a mennu (10 items) for that restaurant. To do this we'll need to pass the request from the first prompt to the next prompt.\n",
    "One way to do so is make separate back-to-back requests, like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please suggest a menu for Gusto's Trattoria. Suggest 10 items as a numbered list\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_menu = PromptTemplate(\n",
    "    # you can have multiple input variables, hence the list\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    # this is your prompt - same as above except the cuisine is parameterised with {}\n",
    "    # similar to a Python f-string\n",
    "    template=\"Please suggest a menu for {restaurant_name}. Suggest 10 items as a numbered list\",\n",
    "    verbose=True,\n",
    ")\n",
    "# here is how you format the template\n",
    "prompt_template_menu.format(restaurant_name=rest_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "1. Spaghetti Carbonara \n",
      "2. Lasagna \n",
      "3. Margherita Pizza \n",
      "4. Eggplant Parmigiana \n",
      "5. Grilled Salmon \n",
      "6. Chicken Parmigiana \n",
      "7. Fettuccine Alfredo \n",
      "8. Bruschetta \n",
      "9. Caprese Salad \n",
      "10. Tiramisu\n"
     ]
    }
   ],
   "source": [
    "chain2 = LLMChain(llm=llm, prompt=prompt_template_menu)\n",
    "response2 = chain2.run(restaurant_name=rest_name)\n",
    "print(response2.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yummy ðŸ¤¤ - no?\n",
    "But this is not a practical way to make multiple requests - we can let LangChain chain several requests together - that's what Langchain does - it's in the name! \n",
    "Let's illustrate this using a **Sequential Chain**, specifically the `SimpleSequentialChain` class, which is a simple prompt chaining mechanism which does essentially what we have done above in 2 separate steps. It chains the input of the 2nd step with the output of the first step and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our llm\n",
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "# prompt template for the restaurant name, given a cuisine\n",
    "prompt_template_name = PromptTemplate(\n",
    "    # you can have multiple input variables, hence the list\n",
    "    input_variables=[\"cuisine\"],\n",
    "    # this is your prompt - same as above except the cuisine is parameterised with {}\n",
    "    # similar to a Python f-string\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for it. Only one name please.\",\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "# prompt template for menu items given a restaurant name\n",
    "prompt_template_menu = PromptTemplate(\n",
    "    # you can have multiple input variables, hence the list\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    # this is your prompt - same as above except the cuisine is parameterised with {}\n",
    "    # similar to a Python f-string\n",
    "    template=\"Please suggest 10-15 menu items for {restaurant_name}. Return as comma separated list\",\n",
    ")\n",
    "\n",
    "menu_chain = LLMChain(llm=llm, prompt=prompt_template_menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Murg Tikka, Tandoori Paneer, Tandoori Fish, Chicken Malai Tikka, Seekh Kebab, Vegetable Samosa, Paneer Tikka, Tandoori Naan, Tawa Paneer, Achari Aloo, Tandoori Aloo, Paneer Tikka Masala, Tandoori Roti, Chicken Biryani, Dal Makhani.\n"
     ]
    }
   ],
   "source": [
    "# SimpleSequential Chain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "simple_seq_chain = SimpleSequentialChain(\n",
    "    chains = [name_chain, menu_chain], # NOTE: sequence matters here!!\n",
    ")\n",
    "# now execute the chain with variable for FIRST item in chain only! That's it!\n",
    "response = simple_seq_chain.run(\"Indian\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Capturing intermediate output in chains\n",
    "The `SimpleSequentialChain` helps us stitch prompts together into a chain, so that output from one prompt is fed into another without us having to explicitly program it. \n",
    "\n",
    "However, the downside is that we do not get intermediate output. What was the name of the restaurant that the first prompt returned? We don't know! If we cant to capture intermediate output too, then we need to use a different chaining mechanism. Here is where `SequentialChain` comes into play. The following section introduces the `SequentialChain` class. \n",
    "\n",
    "**NOTE:** to capture the intermediate output from chain, we need to add an `output key` to the `LLMChain` constructor using the `output_key=XXX` parameter. Otherwise, most the code remains the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our llm\n",
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "# prompt template for the restaurant name, given a cuisine\n",
    "prompt_template_name = PromptTemplate(\n",
    "    # you can have multiple input variables, hence the list\n",
    "    input_variables=[\"cuisine\"],\n",
    "    # this is your prompt - same as above except the cuisine is parameterised with {}\n",
    "    # similar to a Python f-string\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for it. Only one name please.\",\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, \n",
    "    prompt=prompt_template_name,\n",
    "    # here we add an output_key parameter to capture the output from this prompt\n",
    "    output_key=\"restaurant_name\",\n",
    ")\n",
    "\n",
    "# prompt template for menu items given a restaurant name\n",
    "prompt_template_menu = PromptTemplate(\n",
    "    # you can have multiple input variables, hence the list\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    # this is your prompt - same as above except the cuisine is parameterised with {}\n",
    "    # similar to a Python f-string\n",
    "    template=\"Please suggest 10-15 menu items for {restaurant_name}. Return as comma separated list\",\n",
    ")\n",
    "\n",
    "# chain is the same as before\n",
    "menu_chain = LLMChain(llm=llm, \n",
    "    prompt=prompt_template_menu,\n",
    "    # here we add an output_key parameter to capture the output from this prompt\n",
    "    output_key=\"menu_items\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'American', 'restaurant_name': '\\n\\nThe Gilded Grill.', 'menu_items': '\\n\\nGrilled Salmon, Roasted Chicken, Filet Mignon, Rib-Eye Steak, Prime Rib, Lamb Chops, Baked Trout, Crab Cakes, Lobster Tail, Seafood Paella, Grilled Vegetables, Caesar Salad, Potato Gratin, Steamed Asparagus, Mac and Cheese, Chocolate Lava Cake.'}\n"
     ]
    }
   ],
   "source": [
    "# SimpleSequential Chain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "seq_chain = SequentialChain(\n",
    "    chains = [name_chain, menu_chain], # NOTE: sequence matters here!!\n",
    "    input_variables = ['cuisine'],     # input variable to first chain in sequence\n",
    "    # NOTE: here we specify the output variables too\n",
    "    output_variables = ['restaurant_name', 'menu_items'],\n",
    ")\n",
    "\n",
    "# now we execute the chain. Note how the call is a bit different\n",
    "response = seq_chain({'cuisine':'American'}) \n",
    "# NOTE: response from above call is a JSON object with keys for all input variables (just 1 in our case = 'cuisine')\n",
    "# and for all our output variables ('restaurant_name' and 'menu_items')\n",
    "# something like this\n",
    "\"\"\"\n",
    "response = {\n",
    "    'cuisine': 'American', \n",
    "    'restaurant_name': '\\n\\nThe Gilded Grill.', \n",
    "    'menu_items': '\\n\\nGrilled Salmon, Roasted Chicken, Filet Mignon, Rib-Eye Steak, Prime Rib, Lamb Chops, Baked Trout, Crab Cakes, Lobster Tail, Seafood Paella, Grilled Vegetables, Caesar Salad, Potato Gratin, Steamed Asparagus, Mac and Cheese, Chocolate Lava Cake.'\n",
    "}\n",
    "\"\"\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
